\begin{Verbatim}[commandchars=\\\{\}]
	\PYG{err}{“””}\PYG{n}{Weighted} \PYG{n}{Gaussian} \PYG{n}{log} \PYG{n}{likelihood} \PYG{n}{loss} \PYG{n}{function} \PYG{k}{for} \PYG{n}{RL}\PYG{err}{”””}
	\PYG{k}{def} \PYG{n+nf}{custom\PYGZus{}loss\PYGZus{}gaussian}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{):}
	\PYG{c+c1}{\PYGZsh{} Predict mu and sigma with actor network}
	\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma} \PYG{o}{=} \PYG{n}{actor\PYGZus{}network}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
	
	\PYG{c+c1}{\PYGZsh{} Compute Gaussian pdf value}
	\PYG{n}{pdf\PYGZus{}value} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{p}{((}\PYG{n}{action} \PYG{err}{—} \PYG{n}{mu}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{sigma}\PYG{p}{))}\PYG{o}{**}\PYG{l+m+mi}{2}\PYG{p}{)}
	\PYG{o}{*} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{n}{sigma} \PYG{o}{*} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{))}
	
	\PYG{c+c1}{\PYGZsh{} Convert pdf value to log probability}
	\PYG{n}{log\PYGZus{}probability} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{pdf\PYGZus{}value} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{)}
	
	\PYG{c+c1}{\PYGZsh{} Compute weighted loss}
	\PYG{n}{loss\PYGZus{}actor} \PYG{o}{=} \PYG{err}{—} \PYG{n}{reward} \PYG{o}{*} \PYG{n}{log\PYGZus{}probability}
	
	\PYG{k}{return} \PYG{n}{loss\PYGZus{}actor}
\end{Verbatim}
